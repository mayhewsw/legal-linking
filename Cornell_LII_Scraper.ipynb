{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper script, to scrape opinion text from Cornell LII. Also downloads metadata where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import fileinput\n",
    "\n",
    "# base user path - modify as necessary\n",
    "base_path = 'path/to/legal-linking'\n",
    "\n",
    "# base site URL\n",
    "base_url = 'https://www.law.cornell.edu'\n",
    "\n",
    "# type table to translate from single-letter opinion codes to word opinion codes\n",
    "type_table = {'O': 'opinion',\n",
    "              'C': 'concurrence',\n",
    "              'D': 'dissent',\n",
    "              'PC': 'percuriam'}\n",
    "\n",
    "# full output file paths\n",
    "out_file_full = os.path.join(base_path, 'data/all_data/ussc_out_full.json'\n",
    "out_file_stripped = os.path.join(base_path, 'data/all_data/ussc_out_stripped.json'\n",
    "\n",
    "# directory for batched output paths, and batch size\n",
    "split_out_dir = os.path.join(base_path, 'data/all_data/')\n",
    "\n",
    "# dictionary containers for constitution excerpt/index translation tables\n",
    "href_dic = {}\n",
    "index_dic = {}\n",
    "\n",
    "# output files for the two dictionary href/index translation tables\n",
    "href_out = os.path.join(base_path, 'data/href_dic.json')\n",
    "index_out = os.path.join(base_path, 'data/constitution.json')\n",
    "\n",
    "# dictionary container for translating between matches in-text and standard URL links to constitution sections\n",
    "match_link_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _Case:\n",
    "    \"\"\"\n",
    "    Base class for Supreme Court cases. Given HTML from a Cornell LII page, retrieves text and metadata.\n",
    "    For each page, \"processes\" (extracts text data from) the page, and identifies instances where the page\n",
    "    links to a section of the US Constitution. \n",
    "    \n",
    "    Because the Cornell LII database stores cases in multiple formats, class variants inherit this base class\n",
    "    and define distinct process() methods for each scenario. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, page):\n",
    "        self.page = page\n",
    "        self.page_soup = BeautifulSoup(self.page.read())\n",
    "        \n",
    "        self.paragraphs = []\n",
    "        self.paragraphs_stripped = []\n",
    "        \n",
    "        self.process()\n",
    "        \n",
    "        for i, par in enumerate(self.paragraphs):\n",
    "            stripped_par = deepcopy(par)\n",
    "            \n",
    "            for match in stripped_par['matches']:\n",
    "                stripped_par['text'] = re.sub(match[0], '@@@', stripped_par['text'])\n",
    "            \n",
    "            self.paragraphs_stripped.append(stripped_par)\n",
    "        \n",
    "    def process():\n",
    "        \"\"\" Dummy HTML page processing function. \"\"\"\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def _retrieve_data(soup):\n",
    "        \"\"\" \n",
    "        Retrieves text and links to the US Constitution from Cornell LII HTML pages. Also creates \n",
    "        output object. Metadata fields filled in process() functions, since metadata location is \n",
    "        format-dependent.\n",
    "        \n",
    "        NOTE: this setup is definitely not perfect. Some extraneous text (e.g. footnotes) is sometimes\n",
    "        included, and some text that should be included is deleted. Works in most cases, but could \n",
    "        be improved.\n",
    "        \"\"\"\n",
    "        if not soup:\n",
    "            text_tags = []\n",
    "        else:\n",
    "            # first try to find regular 'body' text tags\n",
    "            text_tags = soup.find_all('p', class_='bodytext')\n",
    "            \n",
    "            # then try to find cases where the 'disposition' is marked, and take all the 'p' tags\n",
    "            if not text_tags:\n",
    "                opinion_start = soup.find('disposition')\n",
    "                if opinion_start:\n",
    "                    text_tags = [tag for tag in opinion_start.find_all_next('p')]\n",
    "            \n",
    "            # if that fails, take all non-footnote tags\n",
    "            if not text_tags:\n",
    "                text_tags = soup.find_all(lambda tag: tag.name == 'p' and tag.has_attr('class') and \n",
    "                                          'pro-indent' in tag['class'])\n",
    "        \n",
    "        paragraph_data = [{'text': tag.text, \n",
    "                           'meta': {'doc_type': None, 'id': None, 'source_url': None, 'date': None}, \n",
    "                           'matches': [(match.text, match['href']) for match in \n",
    "                                       tag.find_all('a', href=re.compile('constitution|get-const'))]} \n",
    "                          for tag in text_tags]\n",
    "\n",
    "        return(paragraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCase(_Case):\n",
    "    \"\"\"\n",
    "    Class for text-based cases (i.e. those cases formatted as plain text, without much embedded HTML). \n",
    "    \n",
    "    Note that metadata fields in these cases are inconsistently present. The metadata retrieval steps here \n",
    "    represent common scenarios, but there are a lot of case pages that don't have clear metadata fields \n",
    "    (or any metadata fields at all).\n",
    "    \"\"\"\n",
    "    def process(self):\n",
    "        # some cases have a \"casecontent\" section, that marks opinion content\n",
    "        case_content = self.page_soup.find('casecontent')\n",
    "\n",
    "        if case_content:\n",
    "            date = case_content.find('p', class_ = 'date')\n",
    "            if date:\n",
    "                date = date.text\n",
    "                \n",
    "            doc_content = self._retrieve_data(case_content)\n",
    "            \n",
    "            for par in doc_content:\n",
    "                par['meta']['doc_type'] = None\n",
    "                par['meta']['id'] = len(self.paragraphs)\n",
    "                par['meta']['source_url'] = self.page.url\n",
    "                par['meta']['date'] = date\n",
    "                \n",
    "                self.paragraphs.append(par)\n",
    "            \n",
    "        # otherwise, look for opinion type markers\n",
    "        else: \n",
    "            doc_types = ['percuriam', 'opinion', 'concurrence', 'dissent']\n",
    "\n",
    "            date_tag = self.page_soup.find('div', class_='opiniondates')\n",
    "\n",
    "            if date_tag:\n",
    "                date = date_tag.text\n",
    "            else:\n",
    "                date = None\n",
    "\n",
    "            for doc_type in doc_types:\n",
    "                doc = self.page_soup.find('div', class_ = doc_type)\n",
    "                if doc:\n",
    "                    doc_content = self._retrieve_data(doc)\n",
    "\n",
    "                    for par in doc_content:\n",
    "                        doc_content[i]['meta']['doc_type'] = doc_type\n",
    "                        doc_content[i]['meta']['id'] = len(self.paragraphs)\n",
    "                        doc_content[i]['meta']['source_url'] = self.page.url\n",
    "                        doc_content[i]['meta']['date'] = date\n",
    "\n",
    "                        self.paragraphs.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HtmlCase(_Case):\n",
    "    \"\"\"\n",
    "    Class for HTML-based cases (i.e. those cases with embedded HTML). A key difference from the HTML cases\n",
    "    is that separate opinions are clearly marked, with distinct pages and a distinct letter coding system that\n",
    "    denotes the opinion type (\"O\" for majority opinion, \"C\" for concurrence, \"D\" for dissent, \"PC\" for per \n",
    "    curium). \n",
    "    \n",
    "    Note that metadata fields in these cases are inconsistently present. The metadata retrieval steps here \n",
    "    represent common scenarios, but there are a lot of case pages that don't have clear metadata fields \n",
    "    (or any metadata fields at all).\n",
    "    \"\"\"\n",
    "    def process(self): \n",
    "        html_base_url = 'https://www.law.cornell.edu/supct/html/'\n",
    "        \n",
    "        date_tag = self.page_soup.find('meta', attrs={'name':'DECDATE'})\n",
    "        \n",
    "        if date_tag:\n",
    "            date = date_tag['content']\n",
    "        else:\n",
    "            date = None\n",
    "        \n",
    "        # find the opinion links. Note that all opinion links have one of the opinion letter codes present\n",
    "        opinion_links = [self.page.url]\n",
    "        opinion_links += [urljoin(html_base_url, tag['href']) for tag in \n",
    "                          self.page_soup.find_all(lambda tag: tag.name == 'a' and tag.has_attr('href'))]\n",
    "        opinion_links = filter(lambda url: re.search('Z([OCD]|PC)', url) and 'pdf' not in url and 'html' in url,\n",
    "                               opinion_links)\n",
    "        \n",
    "        for url in opinion_links:\n",
    "            doc_result = urlopen(url)\n",
    "            doc = BeautifulSoup(doc_result.read())\n",
    "            doc_content = self._retrieve_data(doc)\n",
    "            \n",
    "            for par in doc_content:\n",
    "                par['meta']['id'] = len(self.paragraphs)\n",
    "                par['meta']['doc_type'] = type_table[re.search('Z([OCD]|PC)', url).group(1)]\n",
    "                par['meta']['source_url'] = url\n",
    "                par['meta']['date'] = date\n",
    "                \n",
    "                self.paragraphs.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search results base url and results page indices - hard-coded to max pages (verified manually)\n",
    "pages_url = 'https://www.law.cornell.edu/search/site?page={}&f[0]=bundle%3Asupct_node'\n",
    "pages = range(0, 3183)\n",
    "\n",
    "# loop over pages and retrive output\n",
    "# NOTE: output files are much larger than github maximum file size, so consider changing output\n",
    "# location or not pushing uploaded files\n",
    "for page in pages:\n",
    "    page_soup = BeautifulSoup(urlopen(pages_url.format(page)).read())\n",
    "    links = [tag.a for tag in page_soup.find_all('li', class_ = 'search-result')]\n",
    "\n",
    "    for link in links:\n",
    "        print(link)\n",
    "        \n",
    "        try:\n",
    "            # checking for invalid urls\n",
    "            if '%20' in link.get('href'):\n",
    "                continue\n",
    "                \n",
    "            result = urlopen(link.get('href'))\n",
    "            \n",
    "            # checking for invalid redirects\n",
    "            if 'home' in result.url:\n",
    "                continue\n",
    "\n",
    "            # check opinon type\n",
    "            if 'html' in result.url:\n",
    "                case = HtmlCase(result)\n",
    "            else:\n",
    "                case = TextCase(result)\n",
    "\n",
    "            if len(case.paragraphs) == 0:\n",
    "                print('WARNING: NO CONTENT FOUND')\n",
    "\n",
    "            with open(out_file_full, 'a') as f:\n",
    "                f.write(json.dumps(case.paragraphs) + '\\n')\n",
    "\n",
    "            with open(out_file_stripped, 'a') as f:\n",
    "                f.write(json.dumps(case.paragraphs_stripped) + '\\n')\n",
    "        \n",
    "        # generic error handler - shouldn't come up in practice\n",
    "        except:\n",
    "            print('ERROR!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split files into batches and write output, for github uploading convenience\n",
    "\n",
    "with open(out_file_full) as f:\n",
    "    batch = 0\n",
    "    n_pars = 0\n",
    "    \n",
    "    for case in f.readlines():\n",
    "        json_case = json.loads(case)\n",
    "        \n",
    "        n_pars += len(json_case)\n",
    "        out_file = split_out_dir + 'ussc_out_full_' + str(batch) + '.json'\n",
    "\n",
    "        with open(out_file, 'a') as f:\n",
    "            f.write(case)\n",
    "                    \n",
    "        if n_pars > 50000:\n",
    "            batch += 1\n",
    "            n_pars = 0\n",
    "            \n",
    "with open(out_file_stripped) as f:\n",
    "    batch = 0\n",
    "    n_pars = 0\n",
    "    \n",
    "    for case in f.readlines():\n",
    "        json_case = json.loads(case)\n",
    "        \n",
    "        n_pars += len(json_case)\n",
    "        out_file = split_out_dir + 'ussc_out_stripped_' + str(batch) + '.json'\n",
    "\n",
    "        with open(out_file, 'a') as f:\n",
    "            f.write(case)\n",
    "                    \n",
    "        if n_pars > 50000:\n",
    "            batch += 1                \n",
    "            n_pars = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scrape the constitution containers (href_dic, index_dic)\n",
    "# these have identical contents, but different keys:\n",
    "# - href_dic uses hyperlinks as keys\n",
    "# - index_dic uses short names for constitution paragraphs as keys\n",
    "\n",
    "# retrieve all the links on the constitution page in Cornell LII\n",
    "soup = BeautifulSoup(urlopen(base_url + '/constitution'))\n",
    "links = soup.find_all(lambda tag: tag.name == 'a' and tag.has_attr('href') and 'constitution/' in tag.get('href'))\n",
    "links = list(set([base_url + tag.get('href') for tag in links]))\n",
    "\n",
    "# loop over constitution section links\n",
    "for i, link in enumerate(links):\n",
    "    segment_name = re.search('(?<=#).*', link)\n",
    "    link_soup = BeautifulSoup(urlopen(link))\n",
    "    div_tags = link_soup.find_all(lambda tag: tag.name == 'div' and tag.has_attr('property') and \n",
    "                                              tag.get('property') == 'content:encoded')\n",
    "    if not div_tags:\n",
    "        continue\n",
    "    \n",
    "    if segment_name:\n",
    "        div_p_tags = []\n",
    "        for div in div_tags:\n",
    "            p_container = []\n",
    "            \n",
    "            a_tags = [a for a in div.find_all('a', id=segment_name.group(0))]\n",
    "            \n",
    "            for a in a_tags:\n",
    "                next_tag = a.find_next()\n",
    "                while next_tag.name != 'h2': \n",
    "                    if next_tag.name == 'p':\n",
    "                        p_container.append(next_tag)\n",
    "\n",
    "                    next_tag = next_tag.find_next()\n",
    "                    \n",
    "            if p_container:\n",
    "                div_p_tags.append(p_container)\n",
    "    \n",
    "    else:\n",
    "        div_p_tags = [[p for p in div.find_all('p')] for div in div_tags]\n",
    "    \n",
    "    text = '\\n'.join(['\\n'.join([p.text for p in p_container]) for p_container in div_p_tags])\n",
    "    \n",
    "    if not text:\n",
    "        continue\n",
    "        \n",
    "    key = re.search('[^/]+$', link).group(0)\n",
    "\n",
    "    href_dic[link] = {'key': key,\n",
    "                      'index': i,\n",
    "                      'text': text,\n",
    "                      'link': link}\n",
    "\n",
    "    index_dic[key] = {'key': key,\n",
    "                     'index': i,\n",
    "                     'text': text,\n",
    "                     'link': link}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deduplicate\n",
    "index_dic = {id_val: index_dic[id_val] for id_val in index_dic if \n",
    "                     ('article' not in index_dic[id_val]['link']) or\n",
    "                     ('articlev' in index_dic[id_val]['link']) or\n",
    "                     ('article' in index_dic[id_val]['link'] and 'section' in index_dic[id_val]['link'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the containers\n",
    "with open(href_out, 'w') as f:\n",
    "    f.write(json.dumps(href_dic))\n",
    "\n",
    "with open(index_out, 'w') as f:\n",
    "    f.write(json.dumps(index_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add container indicators to the files\n",
    "# original scraped versions of data files just have hyperlinks - here, we give them some more informative links\n",
    "\n",
    "with open(href_out) as f:\n",
    "    href_dic = json.loads(f.read())\n",
    "\n",
    "out_files = [os.path.join(split_out_dir, fname) for fname in os.listdir(split_out_dir) \n",
    "             if 'full' in fname or 'stripped' in fname]\n",
    "\n",
    "count = 0\n",
    "\n",
    "match_link_dic = {}\n",
    "for fname in out_files:\n",
    "    print(fname)\n",
    "    container = []\n",
    "    with open(fname) as f:\n",
    "        for case in f.readlines():\n",
    "            json_case = json.loads(case)\n",
    "            \n",
    "            for i, row in enumerate(json_case):\n",
    "                for j, match in enumerate(row['matches']):\n",
    "                    if match[1] not in match_link_dic:\n",
    "                        url_to_search = match[1]\n",
    "                        if 'cornell' not in match[1]:\n",
    "                            url_to_search = base_url + url_to_search\n",
    "                        \n",
    "                        url_to_search = re.sub('constitution/constitution\\.(billofrights\\.html#)?', \n",
    "                                               'supct-cgi/get-const?', url_to_search)\n",
    "                        url_to_search = re.sub('billofrights\\.html#',\n",
    "                                               '', url_to_search)\n",
    "                        url_to_search = re.sub('\\.html', '', url_to_search)\n",
    "                        \n",
    "                        result = urlopen(url_to_search)\n",
    "                        match_link_dic[match[1]] = result.url\n",
    "                \n",
    "                    match.append(str(href_dic[match_link_dic[match[1]]]['key']))\n",
    "                    json_case[i]['matches'][j] = match\n",
    "                \n",
    "            container.append(json_case)\n",
    "    \n",
    "    with open(fname, 'w') as f:\n",
    "        for case in container:\n",
    "            f.write(json.dumps(case) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
